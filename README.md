# 파이썬 기반 웹 크롤러 및 데이터 시각화 서비스

## 📄 프로젝트 소개

이 프로젝트는 사용자가 지정한 웹사이트의 특정 데이터를 수집(크롤링)하고, 수집된 데이터를 기반으로 간단한 데이터 분석 및 시각화를 수행할 수 있는 웹 서비스입니다. Flask를 백엔드 프레임워크로 사용하며, 프론트엔드에서는 Plotly.js를 활용하여 인터랙티브한 차트를 제공합니다.

## ✨ 주요 기능

* **웹 크롤링:**
    * 사용자가 입력한 URL과 HTML 태그/CSS 선택자를 기반으로 웹 페이지의 텍스트 및 링크 데이터를 수집합니다.
    * 수집된 데이터는 서버의 `crawled_data/` 폴더에 타임스탬프 및 URL 기반의 CSV 파일로 자동 저장됩니다.
* **데이터 시각화:**
    * **탭 기반 UI:** "웹 크롤러" 탭과 "데이터 시각화" 탭으로 기능이 분리되어 사용성을 높였습니다.
    * **CSV 파일 선택:** 저장된 CSV 파일 목록에서 분석할 파일을 선택할 수 있습니다.
    * **키워드 빈도 분석:** 선택된 CSV 파일의 'text' 컬럼에서 사용자가 입력한 키워드들의 빈도수를 분석합니다.
    * **차트 종류 선택:** 분석 결과를 막대 그래프 또는 선 그래프 형태로 선택하여 시각화할 수 있습니다. (Plotly.js 사용)
* **사용자 인터페이스:**
    * 직관적인 웹 UI를 통해 크롤링 요청 및 시각화 옵션 설정을 할 수 있습니다.
    * 크롤링 및 시각화 진행 상태, 결과, 오류 메시지를 사용자에게 명확하게 안내합니다.

## 🛠️ 기술 스택

* **백엔드:**
    * Python 3.x
    * Flask (웹 프레임워크)
    * Pandas (데이터 분석 및 CSV 처리)
    * Beautiful Soup 4 (HTML 파싱)
    * Requests (HTTP 요청)
* **프론트엔드:**
    * HTML5
    * CSS3
    * JavaScript (ES6+)
    * Plotly.js (인터랙티브 차트 라이브러리, CDN 방식 사용)
* **데이터 저장:**
    * 로컬 CSV 파일 (수집된 데이터)

## 📂 프로젝트 구조

simple_crawler/
├── app.py             # Flask 애플리케이션 (백엔드 로직)
├── templates/
│   └── index.html     # 사용자 인터페이스 (프론트엔드 HTML, CSS, JavaScript)
└── crawled_data/      # 크롤링된 CSV 파일 저장 폴더 (자동 생성됨)



## 🚀 설치 및 실행 방법

### 1. 사전 준비 사항
* Python 3.7 이상 설치

### 2. 프로젝트 설정
1.  **저장소 복제 (Clone Repository):**
    ```bash
    git clone <저장소_URL>
    cd simple_crawler
    ```

2.  **가상 환경 생성 및 활성화 (권장):**
    ```bash
    # macOS / Linux
    python3 -m venv venv
    source venv/bin/activate

    # Windows
    python -m venv venv
    .\venv\Scripts\activate
    ```

3.  **필수 라이브러리 설치:**
    프로젝트 루트 디렉토리(`simple_crawler/`)에 다음 내용으로 `requirements.txt` 파일을 생성합니다:
    ```txt
    Flask
    requests
    beautifulsoup4
    pandas
    ```
    그 다음, 터미널에서 아래 명령어를 실행하여 라이브러리를 설치합니다:
    ```bash
    pip install -r requirements.txt
    ```

### 3. 애플리케이션 실행
1.  프로젝트 루트 디렉토리(`simple_crawler/`)에서 다음 명령어를 실행하여 Flask 개발 서버를 시작합니다:
    ```bash
    python app.py
    ```
2.  웹 브라우저를 열고 다음 주소로 접속합니다:
    `http://127.0.0.1:5100/` (또는 `app.py`에 설정된 다른 포트 번호)

## 📖 사용 가이드

### 데이터 수집 (웹 크롤러 탭)
1.  "웹사이트 URL" 입력란에 크롤링할 대상 웹 페이지의 전체 주소를 입력합니다.
2.  "추출할 HTML 태그 또는 CSS 선택자" 입력란에 수집하고자 하는 데이터가 포함된 HTML 요소의 태그명(예: `h2`, `p a`) 또는 CSS 선택자(예: `div.content-area`, `#main-title span`)를 입력합니다.
3.  "크롤링 시작" 버튼을 클릭합니다.
4.  크롤링이 완료되면 결과 메시지와 함께 수집된 데이터의 일부가 화면에 표시됩니다.
5.  전체 데이터는 서버의 `crawled_data/` 폴더 내에 CSV 파일로 저장되며, 저장된 파일 경로가 안내됩니다.

### 데이터 시각화 (데이터 시각화 탭)
1.  "데이터 시각화" 탭으로 이동합니다. (탭 이동 시 CSV 파일 목록이 자동으로 로드됩니다.)
2.  "분석할 CSV 파일 선택" 드롭다운 메뉴에서 이전에 크롤링하여 저장된 CSV 파일을 선택합니다.
3.  "분석 유형 선택"에서 현재는 "키워드 빈도 분석"만 제공됩니다.
4.  "키워드 입력 (쉼표로 구분)"란에 분석하고자 하는 키워드들을 쉼표(,)로 구분하여 입력합니다. (예: `기술, AI, 데이터`)
5.  "차트 종류 선택"에서 "막대 그래프" 또는 "선 그래프" 중 원하는 형태를 선택합니다.
6.  "시각화 생성" 버튼을 클릭합니다.
7.  선택한 옵션에 따라 분석된 결과가 차트 형태로 하단에 표시됩니다.

## 🔧 문제 해결 (Troubleshooting)

* **"Address already in use" / "Port ... is in use" 오류 발생 시:**
    * `app.py` 파일 하단의 `app.run(debug=True, port=5100)` 부분에서 `port` 번호를 다른 번호(예: 5101, 5200)로 변경해보세요.
    * 또는, 해당 포트를 이미 사용 중인 다른 프로그램을 찾아 종료해야 합니다. (macOS/Linux: `sudo lsof -i :<포트번호>`로 확인 후 `sudo kill -9 <PID>`)
* **`jinja2.exceptions.TemplateNotFound: index.html` 오류 발생 시:**
    * `app.py` 파일과 동일한 위치에 `templates`라는 이름의 폴더가 있는지, 그리고 그 폴더 안에 `index.html` 파일이 정확히 위치해 있는지 확인하세요.
* **JavaScript 오류 또는 UI 요소가 제대로 동작하지 않을 시:**
    * 브라우저의 개발자 도구(F12 또는 Ctrl/Cmd+Shift+I)를 열어 "콘솔(Console)" 탭에서 오류 메시지를 확인하세요.
    * `index.html` 파일 내의 HTML 요소 ID와 JavaScript 코드에서 참조하는 ID가 일치하는지, 파일이 최신 버전인지 확인하세요.

## 🚀 향후 개선 사항 (TODO)

* **다양한 분석 메뉴 추가:**
    * 시간에 따른 데이터 변화 분석 (CSV 내 타임스탬프 컬럼 필요)
    * 데이터 항목별 관계 분석 등
* **차트 종류 확장:** 원형 그래프, 산점도 등 Plotly.js가 지원하는 다양한 차트 옵션 제공
* **데이터 컬럼 선택 기능:** 사용자가 CSV 파일 내 특정 컬럼을 분석 대상으로 선택할 수 있도록 기능 추가
* **데이터베이스 연동:** 수집된 데이터를 CSV 파일 대신 데이터베이스(예: SQLite, PostgreSQL)에 저장하고 관리
* **사용자 인증 및 작업 관리:** 사용자별 크롤링/시각화 작업 이력 관리
* **비동기 크롤링 작업 처리 개선:** 대규모 크롤링 시 Celery 등 작업 큐 도입 고려
* **UI/UX 개선:** 프론트엔드 프레임워크(Vue.js, React 등) 도입 고려

---
